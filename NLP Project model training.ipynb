{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13487163,"sourceType":"datasetVersion","datasetId":8562961}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ================================================================\n# ‚úÖ AMAZON REVIEWS SENTIMENT CLASSIFIER (ROBERTA-BASE)\n# ================================================================\n\nimport os\nimport json\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    TextClassificationPipeline,\n)\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# ================================================================\n# üß© LOAD AMAZON JSONL DATA\n# ================================================================\ndef load_amazon_jsonl(folder_path, max_samples=5000):\n    data = []\n    for file in os.listdir(folder_path):\n        if file.endswith(\".jsonl\"):\n            path = os.path.join(folder_path, file)\n            print(f\"üìÇ Loading: {file}\")\n            with open(path, \"r\", encoding=\"utf-8\") as f:\n                for line in f:\n                    try:\n                        obj = json.loads(line)\n                        text = (\n                            obj.get(\"reviewText\")\n                            or obj.get(\"review_text\")\n                            or obj.get(\"review_body\")\n                            or obj.get(\"text\")\n                        )\n                        rating = obj.get(\"overall\") or obj.get(\"rating\")\n                        if text and rating:\n                            data.append({\n                                \"text\": text.strip(),\n                                \"label\": int(rating) - 1  # labels 0‚Äì4\n                            })\n                        if len(data) >= max_samples:\n                            break\n                    except json.JSONDecodeError:\n                        continue\n    df = pd.DataFrame(data)\n    print(f\"‚úÖ Loaded {len(df)} samples.\")\n    print(\"Sample rows:\\n\", df.head(2).to_dict(orient=\"records\"))\n    print(\"Label distribution:\\n\", df['label'].value_counts().to_dict())\n    return df\n\n\n# ================================================================\n# ‚öôÔ∏è TRAINER CLASS\n# ================================================================\nclass SentimentTrainer:\n    def __init__(self, model_name=\"roberta-base\"):\n        self.model_name = model_name\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        print(f\"üöÄ Device: {self.device}\")\n\n    def prepare(self, num_labels=5):\n        print(f\"üöÄ Loading tokenizer and model: {self.model_name}\")\n        tok = AutoTokenizer.from_pretrained(\n            self.model_name, use_fast=True, local_files_only=False\n        )\n        model = AutoModelForSequenceClassification.from_pretrained(\n            self.model_name, num_labels=num_labels\n        )\n        return model.to(self.device), tok\n\n    def tokenize(self, examples, tokenizer):\n        return tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=128,\n        )\n\n    def train(self, model, tokenizer, train_df, eval_df):\n        print(\"üîÅ Tokenizing train dataset (num_proc=1)...\")\n        train_ds = Dataset.from_pandas(train_df)\n        eval_ds = Dataset.from_pandas(eval_df)\n\n        train_ds = train_ds.map(lambda e: self.tokenize(e, tokenizer), batched=True)\n        eval_ds = eval_ds.map(lambda e: self.tokenize(e, tokenizer), batched=True)\n\n        columns = [\"input_ids\", \"attention_mask\", \"label\"]\n        train_ds.set_format(type=\"torch\", columns=columns)\n        eval_ds.set_format(type=\"torch\", columns=columns)\n\n        args = TrainingArguments(\n            output_dir=\"./results\",\n            eval_strategy=\"steps\",\n            save_strategy=\"epoch\",\n            learning_rate=2e-5,\n            per_device_train_batch_size=8,\n            per_device_eval_batch_size=8,\n            num_train_epochs=3,\n            weight_decay=0.01,\n            logging_dir=\"./logs\",\n            logging_steps=50,\n            report_to=\"none\",\n            disable_tqdm=False,\n        )\n\n        trainer = Trainer(\n            model=model,\n            args=args,\n            train_dataset=train_ds,\n            eval_dataset=eval_ds,\n        )\n\n        print(\"‚úÖ Pre-training checks passed. Starting Trainer.train() now.\")\n        trainer.train()\n        print(\"‚úÖ Trainer finished.\")\n        return trainer\n\n\n# ================================================================\n# üìä EVALUATION FUNCTION\n# ================================================================\ndef evaluate_model(trainer, eval_df, tokenizer, model):\n    eval_ds = Dataset.from_pandas(eval_df)\n    eval_ds = eval_ds.map(lambda e: tokenizer(e[\"text\"], truncation=True, padding=\"max_length\", max_length=128), batched=True)\n    eval_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\n    preds = trainer.predict(eval_ds)\n    preds_logits = preds.predictions\n    preds_labels = np.argmax(preds_logits, axis=1)\n    true_labels = preds.label_ids\n\n    acc = accuracy_score(true_labels, preds_labels)\n    f1 = f1_score(true_labels, preds_labels, average=\"weighted\")\n    print(\"\\nüìà Evaluation Results:\")\n    print(f\"Accuracy: {acc:.4f}\")\n    print(f\"Weighted F1: {f1:.4f}\")\n    print(\"\\nDetailed Classification Report:\\n\")\n    print(classification_report(true_labels, preds_labels))\n    return acc, f1\n\n\n# ================================================================\n# üí¨ PREDICTION FUNCTION\n# ================================================================\ndef predict_sentiment(texts, tokenizer, model):\n    pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n    label_map = {\n        0: \"‚≠ê Terrible\",\n        1: \"‚≠ê Bad\",\n        2: \"‚≠ê‚≠ê Neutral\",\n        3: \"‚≠ê‚≠ê‚≠ê Good\",\n        4: \"‚≠ê‚≠ê‚≠ê‚≠ê Excellent\"\n    }\n    outputs = pipe(texts, truncation=True)\n    print(\"\\nüí¨ Predictions:\")\n    for t, o in zip(texts, outputs):\n        label_id = int(o[\"label\"].split(\"_\")[-1]) if \"_\" in o[\"label\"] else np.argmax(o[\"score\"])\n        sentiment = label_map.get(label_id, f\"Label {label_id}\")\n        print(f\"\\nüìù Text: {t}\\nüîπ Sentiment: {sentiment} ({o['score']:.2f})\")\n\n\n# ================================================================\n# üíæ SAVE / LOAD MODEL HELPERS\n# ================================================================\ndef save_model(trainer, tokenizer, save_dir=\"./roberta_sentiment\"):\n    os.makedirs(save_dir, exist_ok=True)\n    trainer.save_model(save_dir)\n    tokenizer.save_pretrained(save_dir)\n    print(f\"üíæ Model and tokenizer saved to {save_dir}\")\n\ndef load_model(save_dir=\"./roberta_sentiment\"):\n    model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n    tokenizer = AutoTokenizer.from_pretrained(save_dir)\n    print(f\"üì¶ Model loaded from {save_dir}\")\n    return model, tokenizer\n\n\n# ================================================================\n# üß† MAIN EXECUTION\n# ================================================================\ndef main(data_path, max_samples=5000):\n    df = load_amazon_jsonl(data_path, max_samples)\n    if len(df) == 0:\n        raise ValueError(\"No data loaded. Check your JSONL keys or folder path.\")\n    train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n\n    trainer_obj = SentimentTrainer(\"roberta-base\")\n    model, tokenizer = trainer_obj.prepare(num_labels=5)\n    trainer = trainer_obj.train(model, tokenizer, train_df, eval_df)\n\n    # Evaluate\n    evaluate_model(trainer, eval_df, tokenizer, model)\n\n    # Predict sample texts\n    sample_texts = [\n        \"This product is amazing, I absolutely love it!\",\n        \"Terrible quality, completely disappointed.\",\n        \"It‚Äôs okay, not too bad but not great either.\"\n    ]\n    predict_sentiment(sample_texts, tokenizer, model)\n\n    # Save model\n    save_model(trainer, tokenizer)\n\n\n# ================================================================\n# üöÄ RUN\n# ================================================================\nif __name__ == \"__main__\":\n    main(\"/kaggle/input/amazonreviews\", max_samples=50000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T13:25:53.613709Z","iopub.execute_input":"2025-11-01T13:25:53.614139Z","iopub.status.idle":"2025-11-01T16:32:09.921343Z","shell.execute_reply.started":"2025-11-01T13:25:53.614115Z","shell.execute_reply":"2025-11-01T16:32:09.920414Z"}},"outputs":[{"name":"stderr","text":"2025-11-01 13:25:55.617040: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762003555.639591     139 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762003555.646368     139 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"üìÇ Loading: All_Beauty.jsonl\nüìÇ Loading: Appliances.jsonl\nüìÇ Loading: Health_and_Personal_Care.jsonl\n‚úÖ Loaded 50002 samples.\nSample rows:\n [{'text': \"This spray is really nice. It smells really good, goes on really fine, and does the trick. I will say it feels like you need a lot of it though to get the texture I want. I have a lot of hair, medium thickness. I am comparing to other brands with yucky chemicals so I'm gonna stick with this. Try it!\", 'label': 4}, {'text': 'This product does what I need it to do, I just wish it was odorless or had a soft coconut smell. Having my head smell like an orange coffee is offputting. (granted, I did know the smell was described but I was hoping it would be light)', 'label': 3}]\nLabel distribution:\n {4: 30161, 3: 6875, 0: 5355, 2: 4598, 1: 3013}\nüöÄ Device: cuda\nüöÄ Loading tokenizer and model: roberta-base\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"655c790a5fc44c4ab9fbd52d8dbbb401"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fa6f9890e3040dab669ffd21518ebdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03d336cea9d54a21bf5a65d9aee86c03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eaf1de43f774204bc299ac0fbd4b6d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91164ae83a3247349db3aa26347b66c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"949fd4d9b39348368e4d81c8e6da7539"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"üîÅ Tokenizing train dataset (num_proc=1)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e54bc5e34e2b4b0b9d654ce498f43114"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a1b22367c5b4bed90012855c5c7b5a0"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Pre-training checks passed. Starting Trainer.train() now.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7503' max='7503' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7503/7503 3:04:55, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.211400</td>\n      <td>0.930951</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.956100</td>\n      <td>0.809791</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.841300</td>\n      <td>0.781279</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.807900</td>\n      <td>0.739547</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.773600</td>\n      <td>0.773079</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.778400</td>\n      <td>0.713349</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.753300</td>\n      <td>0.712990</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.771400</td>\n      <td>0.683276</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.739400</td>\n      <td>0.682257</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.764800</td>\n      <td>0.699761</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.773200</td>\n      <td>0.724817</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.764100</td>\n      <td>0.677211</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.700800</td>\n      <td>0.678808</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.745000</td>\n      <td>0.678736</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.686800</td>\n      <td>0.693306</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.716000</td>\n      <td>0.669784</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.701500</td>\n      <td>0.676549</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.730600</td>\n      <td>0.658365</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.760800</td>\n      <td>0.680208</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.722900</td>\n      <td>0.676983</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.713100</td>\n      <td>0.699607</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.694000</td>\n      <td>0.667974</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.725200</td>\n      <td>0.654830</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.682000</td>\n      <td>0.656669</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.648900</td>\n      <td>0.662801</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.727000</td>\n      <td>0.645071</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.667400</td>\n      <td>0.671288</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.649800</td>\n      <td>0.681871</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.674100</td>\n      <td>0.704673</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.733000</td>\n      <td>0.669574</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.713200</td>\n      <td>0.666141</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.642100</td>\n      <td>0.648362</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.674300</td>\n      <td>0.659121</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.704400</td>\n      <td>0.659285</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.644100</td>\n      <td>0.654174</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.658400</td>\n      <td>0.645215</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.712600</td>\n      <td>0.670983</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.733000</td>\n      <td>0.643764</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.737800</td>\n      <td>0.719208</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.653300</td>\n      <td>0.645138</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.709400</td>\n      <td>0.651411</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.720300</td>\n      <td>0.643358</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.648200</td>\n      <td>0.648830</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.701000</td>\n      <td>0.641476</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.703400</td>\n      <td>0.647252</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.652900</td>\n      <td>0.641370</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.639400</td>\n      <td>0.639901</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.625000</td>\n      <td>0.653290</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.680700</td>\n      <td>0.651499</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.684500</td>\n      <td>0.650823</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.608700</td>\n      <td>0.667431</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.573800</td>\n      <td>0.655113</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.616300</td>\n      <td>0.647842</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.589600</td>\n      <td>0.661863</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.588600</td>\n      <td>0.660812</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.612800</td>\n      <td>0.638718</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.620200</td>\n      <td>0.642824</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.576400</td>\n      <td>0.649047</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.650700</td>\n      <td>0.639350</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.608700</td>\n      <td>0.638184</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>0.591500</td>\n      <td>0.637104</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.525500</td>\n      <td>0.658276</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>0.678500</td>\n      <td>0.647952</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.626400</td>\n      <td>0.640425</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>0.585500</td>\n      <td>0.664136</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.632000</td>\n      <td>0.646487</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>0.614100</td>\n      <td>0.657591</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.571200</td>\n      <td>0.649611</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>0.561200</td>\n      <td>0.670691</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.607200</td>\n      <td>0.653036</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>0.624200</td>\n      <td>0.648466</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.568400</td>\n      <td>0.656703</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>0.625300</td>\n      <td>0.665447</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.607000</td>\n      <td>0.658438</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>0.639900</td>\n      <td>0.639171</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.599500</td>\n      <td>0.642863</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>0.547500</td>\n      <td>0.654270</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.600300</td>\n      <td>0.643086</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>0.616700</td>\n      <td>0.641527</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.586400</td>\n      <td>0.648114</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>0.577300</td>\n      <td>0.643903</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.591600</td>\n      <td>0.634502</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>0.616700</td>\n      <td>0.642815</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.575900</td>\n      <td>0.633261</td>\n    </tr>\n    <tr>\n      <td>4250</td>\n      <td>0.625800</td>\n      <td>0.627452</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.576900</td>\n      <td>0.638321</td>\n    </tr>\n    <tr>\n      <td>4350</td>\n      <td>0.588400</td>\n      <td>0.640656</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.611800</td>\n      <td>0.637526</td>\n    </tr>\n    <tr>\n      <td>4450</td>\n      <td>0.548000</td>\n      <td>0.657704</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.605000</td>\n      <td>0.660706</td>\n    </tr>\n    <tr>\n      <td>4550</td>\n      <td>0.652300</td>\n      <td>0.631108</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.586800</td>\n      <td>0.636824</td>\n    </tr>\n    <tr>\n      <td>4650</td>\n      <td>0.586300</td>\n      <td>0.650553</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.609700</td>\n      <td>0.647608</td>\n    </tr>\n    <tr>\n      <td>4750</td>\n      <td>0.575500</td>\n      <td>0.647658</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.552300</td>\n      <td>0.645711</td>\n    </tr>\n    <tr>\n      <td>4850</td>\n      <td>0.621300</td>\n      <td>0.643501</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.613100</td>\n      <td>0.653093</td>\n    </tr>\n    <tr>\n      <td>4950</td>\n      <td>0.622600</td>\n      <td>0.626282</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.607700</td>\n      <td>0.643311</td>\n    </tr>\n    <tr>\n      <td>5050</td>\n      <td>0.519600</td>\n      <td>0.644848</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.503900</td>\n      <td>0.661885</td>\n    </tr>\n    <tr>\n      <td>5150</td>\n      <td>0.470700</td>\n      <td>0.680676</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.557200</td>\n      <td>0.657177</td>\n    </tr>\n    <tr>\n      <td>5250</td>\n      <td>0.501600</td>\n      <td>0.660027</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.512000</td>\n      <td>0.661600</td>\n    </tr>\n    <tr>\n      <td>5350</td>\n      <td>0.539500</td>\n      <td>0.652486</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.541300</td>\n      <td>0.661102</td>\n    </tr>\n    <tr>\n      <td>5450</td>\n      <td>0.529000</td>\n      <td>0.658327</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.558100</td>\n      <td>0.659468</td>\n    </tr>\n    <tr>\n      <td>5550</td>\n      <td>0.467200</td>\n      <td>0.666746</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.480100</td>\n      <td>0.671400</td>\n    </tr>\n    <tr>\n      <td>5650</td>\n      <td>0.558900</td>\n      <td>0.663362</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>0.500200</td>\n      <td>0.657732</td>\n    </tr>\n    <tr>\n      <td>5750</td>\n      <td>0.536600</td>\n      <td>0.666983</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.493700</td>\n      <td>0.669963</td>\n    </tr>\n    <tr>\n      <td>5850</td>\n      <td>0.525700</td>\n      <td>0.665842</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>0.484000</td>\n      <td>0.666785</td>\n    </tr>\n    <tr>\n      <td>5950</td>\n      <td>0.567300</td>\n      <td>0.661344</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.514000</td>\n      <td>0.662183</td>\n    </tr>\n    <tr>\n      <td>6050</td>\n      <td>0.507800</td>\n      <td>0.666245</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>0.542600</td>\n      <td>0.663601</td>\n    </tr>\n    <tr>\n      <td>6150</td>\n      <td>0.509200</td>\n      <td>0.665339</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>0.533300</td>\n      <td>0.666807</td>\n    </tr>\n    <tr>\n      <td>6250</td>\n      <td>0.508300</td>\n      <td>0.665204</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>0.489400</td>\n      <td>0.662703</td>\n    </tr>\n    <tr>\n      <td>6350</td>\n      <td>0.497600</td>\n      <td>0.667758</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>0.529900</td>\n      <td>0.669805</td>\n    </tr>\n    <tr>\n      <td>6450</td>\n      <td>0.550400</td>\n      <td>0.667301</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.497400</td>\n      <td>0.669662</td>\n    </tr>\n    <tr>\n      <td>6550</td>\n      <td>0.484700</td>\n      <td>0.676273</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>0.484400</td>\n      <td>0.671413</td>\n    </tr>\n    <tr>\n      <td>6650</td>\n      <td>0.491600</td>\n      <td>0.669110</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>0.553200</td>\n      <td>0.665039</td>\n    </tr>\n    <tr>\n      <td>6750</td>\n      <td>0.533500</td>\n      <td>0.659229</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>0.511700</td>\n      <td>0.663861</td>\n    </tr>\n    <tr>\n      <td>6850</td>\n      <td>0.500400</td>\n      <td>0.664875</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>0.472600</td>\n      <td>0.668475</td>\n    </tr>\n    <tr>\n      <td>6950</td>\n      <td>0.515900</td>\n      <td>0.670016</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.479100</td>\n      <td>0.669876</td>\n    </tr>\n    <tr>\n      <td>7050</td>\n      <td>0.494800</td>\n      <td>0.669601</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>0.532800</td>\n      <td>0.666369</td>\n    </tr>\n    <tr>\n      <td>7150</td>\n      <td>0.478000</td>\n      <td>0.667056</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>0.532100</td>\n      <td>0.665878</td>\n    </tr>\n    <tr>\n      <td>7250</td>\n      <td>0.508300</td>\n      <td>0.667059</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>0.537100</td>\n      <td>0.664601</td>\n    </tr>\n    <tr>\n      <td>7350</td>\n      <td>0.479600</td>\n      <td>0.666088</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>0.534000</td>\n      <td>0.665640</td>\n    </tr>\n    <tr>\n      <td>7450</td>\n      <td>0.501200</td>\n      <td>0.665068</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.504500</td>\n      <td>0.665166</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"‚úÖ Trainer finished.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dc5ce89b65b46a197331c31df1d3e5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nüìà Evaluation Results:\nAccuracy: 0.7575\nWeighted F1: 0.7474\n\nDetailed Classification Report:\n\n              precision    recall  f1-score   support\n\n           0       0.70      0.75      0.72      1070\n           1       0.36      0.31      0.34       597\n           2       0.46      0.49      0.47       899\n           3       0.51      0.36      0.42      1399\n           4       0.88      0.93      0.91      6036\n\n    accuracy                           0.76     10001\n   macro avg       0.58      0.57      0.57     10001\nweighted avg       0.74      0.76      0.75     10001\n\n\nüí¨ Predictions:\n\nüìù Text: This product is amazing, I absolutely love it!\nüîπ Sentiment: ‚≠ê‚≠ê‚≠ê‚≠ê Excellent (0.99)\n\nüìù Text: Terrible quality, completely disappointed.\nüîπ Sentiment: ‚≠ê Terrible (0.96)\n\nüìù Text: It‚Äôs okay, not too bad but not great either.\nüîπ Sentiment: ‚≠ê‚≠ê Neutral (0.85)\nüíæ Model and tokenizer saved to ./roberta_sentiment\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install transformers==4.43.3\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y peft\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install peft==0.10.0\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers, peft, accelerate\nprint(\"Transformers:\", transformers.__version__)\nprint(\"PEFT:\", peft.__version__)\nprint(\"Accelerate:\", accelerate.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T13:25:45.567248Z","iopub.execute_input":"2025-11-01T13:25:45.567845Z","iopub.status.idle":"2025-11-01T13:25:48.048327Z","shell.execute_reply.started":"2025-11-01T13:25:45.567819Z","shell.execute_reply":"2025-11-01T13:25:48.047482Z"}},"outputs":[{"name":"stdout","text":"Transformers: 4.43.3\nPEFT: 0.10.0\nAccelerate: 0.33.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip uninstall -y transformers peft accelerate\n!pip cache purge\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers==4.43.3 peft==0.10.0 accelerate==0.33.0 --no-deps\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}